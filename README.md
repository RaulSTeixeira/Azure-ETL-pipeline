# Data Engineer Foundations: Azure ETL pipeline

## Introduction

The aim of this project is to create an ETL pipeline to first extract data from a datasource, then transform and aggregate the data so that analytics can be derived easily and finally load the transformed data into a SQL database.

This pipeline will use tools from Azure, including Data Factory, Databricks, Data Lake Storage Gen2 and Synapse Analytics. PowerBi might be added in the future, for visualizations.

To keep things simple I've added the source data to github, in csv format, from where Data Factory can connect and extract the necessary data.

## Table of Contents

- [Arquitecture overview](#Arquitecture-Overview)
- [Used data](#DataSource)
- [Extract data from github using Data Factory](#Data-Ingestion)
- [Data Storage using Data Lake Storage Gen 2](#Data-Storage)
- [Transform data using Databricks](#Data-Transformation)
- [Load data with Synapse Analytics](#Load-Data-into-SQL-Database)
- [Data analys](#Data-Analysis)


## Arquitecture Overview

![Pipeline2 drawio (1)](https://github.com/RaulSTeixeira/Azure-tokyo-olympics-project/assets/118553146/1ce08a90-a100-4a06-bbdf-edf539824b56)

## Data Source
The data used in this project is from Tokyo Olympics 2021, available at: https://www.kaggle.com/datasets/arjunprasadsarkhel/2021-olympics-in-tokyo

## Data Ingestion

### Data Storage

## Data Transformation

### Data Analysis

## Load Data into SQL Database
